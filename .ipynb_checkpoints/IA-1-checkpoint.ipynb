{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d437bd6e-a4b8-48cd-8d7f-a12d52f17f01",
   "metadata": {},
   "source": [
    "# <center>Inteligencia Artificial</center>\n",
    "#### <center>Profesor: Dr. Hernán D. Merlino </center>\n",
    "\n",
    "\n",
    "## <center>Trabajo Práctico:</center>\n",
    "### <center>  Generar chistes</center>\n",
    "\n",
    "\n",
    "#### **Integrante:**\n",
    "\n",
    "* Brayan Ricaldi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea8907-9c01-4e36-bbd0-54f77f431cfd",
   "metadata": {},
   "source": [
    "## Explicacion previa:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f33e335-27a6-48ec-b197-a80bbae73039",
   "metadata": {},
   "source": [
    "Utilizare el tipo de dato Arrow y no Pandas. Por los siguientes motivos:\n",
    "   - Arrow esta diseñado para el procesamiento de datos de alto rendimiento y representa  cada conjunto de datos similar a una tabla con un formato en columna de memoria.\n",
    "   - Permite multiplces procesos sin mover ni copiar los datos, estoo hace que iterar los datos sea realmente rapido.\n",
    "   \n",
    "Por estos motivos he podido crear mi modelo y estudiarlo, sin problemas de memoria.<br>\n",
    "Para mas detalles: https://huggingface.co/course/chapter5/4?fw=pt.\n",
    "\n",
    "Team Arrow!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc54ec04-ab31-4309-ac78-73aca16879ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 19:08:46.999961: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7fb8a6-127b-4369-8081-b8c1eeea7bc5",
   "metadata": {},
   "source": [
    "## Descargando los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045dfee-0be0-4c78-84b2-fad6bdb6d5ae",
   "metadata": {},
   "source": [
    "<p>Para este modelo usare 2 conjuntos de datos y los voy a concatenar.</p>\n",
    "<p>Fuentes:</p>\n",
    "<ul>\n",
    "    <li><b>ds_1</b>: https://www.kaggle.com/datasets/abhinavmoudgil95/short-jokes?resource=download</li>\n",
    "    <li><b>ds_2</b>: https://www.kaggle.com/datasets/averkij/reddit-jokes-dataset </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7155887-c767-48a9-9876-ee93f070f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476521cd-95f4-4bcd-bc70-b4724a5ae5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-9313eabec7887a97\n",
      "Found cached dataset csv (/home/br/.cache/huggingface/datasets/csv/default-9313eabec7887a97/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ID', 'Joke'],\n",
       "    num_rows: 231657\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_1 = load_dataset(\n",
    "                    \"csv\",\n",
    "                    data_files=\"./DataSets/shortjokes.csv\",\n",
    "                    split=\"train\"\n",
    "                    )\n",
    "ds_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e69ac4-503b-4fa1-9a2a-3d89ede4473b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': [1, 2],\n",
       " 'Joke': ['[me narrating a documentary about narrators] \"I can\\'t hear what they\\'re saying cuz I\\'m talking\"',\n",
       "  'Telling my daughter garlic is good for you. Good immune system and keeps pests away.Ticks, mosquitos, vampires... men.']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34cd4ad1-6fff-413b-970e-a456ff7792ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/br/.cache/huggingface/datasets/csv/default-9313eabec7887a97/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-751734f7ad0b3d4e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> ID: 133560\n",
      ">>> Joke: I'm not John Madden, just John Disappointeden.\n",
      "\n",
      ">>> ID: 229327\n",
      ">>> Joke: Scientists have recently discovered the existence of a mentally unstable microscopic parasite on the moon... Apparently it's a real lunatic\n"
     ]
    }
   ],
   "source": [
    "#Formato usado: Apache Arrow\n",
    "muestra = ds_1.shuffle(seed=0).select(range(2))\n",
    "\n",
    "for i in muestra:\n",
    "    print(f\"\\n>>> ID: {i['ID']}\")\n",
    "    print(f\">>> Joke: {i['Joke']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8966a0-4188-40a7-8e8a-9984d7531ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-1c4bc933cdb5722b\n",
      "Found cached dataset json (/home/br/.cache/huggingface/datasets/json/default-1c4bc933cdb5722b/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['body', 'id', 'score', 'title'],\n",
       "    num_rows: 194553\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_2 = load_dataset(\n",
    "                    \"json\", \n",
    "                    data_files=\"./DataSets/reddit_jokes.json\", \n",
    "                    split=\"train\"\n",
    "                    )\n",
    "ds_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "213858bc-3439-4ac5-8bc2-d1c860b3f4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/br/.cache/huggingface/datasets/json/default-1c4bc933cdb5722b/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5121b6fde057de41.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Body: A paragraph because hes not quite yet an essay.\n",
      ">>> Id: 32viwk\n",
      ">>> Score: 19\n",
      ">>> Title: What do you call a short Mexican?\n",
      "\n",
      ">>> Body: The circus has cunning stunts.\n",
      ">>> Id: 5r6xj3\n",
      ">>> Score: 1\n",
      ">>> Title: What's the difference between a circus and a strip club?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "muestra = ds_2.shuffle(seed=0).select(range(2))\n",
    "\n",
    "for i in muestra:\n",
    "    print(f\">>> Body: {i['body']}\")\n",
    "    print(f\">>> Id: {i['id']}\")\n",
    "    print(f\">>> Score: {i['score']}\")\n",
    "    print(f\">>> Title: {i['title']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773cc56-12a5-45ad-ab9d-dc2cc9cbbd6d",
   "metadata": {},
   "source": [
    "Para concatenarlos, debo tener los mismos features en los mismos dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c3bb30-c076-4748-be19-a44e5db61f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino los feature id, score, tittle de ds_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3553dd77-3dbe-4bb0-ac6c-3c9e31619b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino los feature id, score, tittle de ds_2\n",
    "ds_2 = ds_2.remove_columns([\"score\", \"title\", \"id\"])\n",
    "#Cambio el nombre body por Joke\n",
    "ds_2.rename_column(\"body\", \"joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ba26a-98c2-43cc-9f8c-022ca108e525",
   "metadata": {},
   "source": [
    "Ahora creare el feature id para ds_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1451eeca-a333-490d-beb5-5bbf8079c520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231657"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_id = len(ds_1)\n",
    "i_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd657c0b-74bf-434a-a7ac-3f79895bbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_id(chiste):\n",
    "    i_id = i_id + 1\n",
    "    return {\"id\": i_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4edce66-2e5c-4598-b7de-6bcc0737ae5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a41295397c4627868acac6da7065e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194553 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'i_id' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24915/2710091776.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrear_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mds_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2585\u001b[0;31m             return self._map_single(\n\u001b[0m\u001b[1;32m   2586\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2587\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m         }\n\u001b[1;32m    551\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2965\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2966\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2967\u001b[0;31m                         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_function_on_filtered_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2968\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2969\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2863\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2864\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2865\u001b[0;31m             \u001b[0mprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2866\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2867\u001b[0m                 \u001b[0;31m# Check if the function returns updated examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2543\u001b[0m                 )\n\u001b[1;32m   2544\u001b[0m                 \u001b[0;31m# Use the LazyDict internally, while mapping the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2545\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorated_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2546\u001b[0m                 \u001b[0;31m# Return a standard dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2547\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24915/1301917372.py\u001b[0m in \u001b[0;36mcrear_id\u001b[0;34m(chiste)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcrear_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchiste\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mi_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'i_id' referenced before assignment"
     ]
    }
   ],
   "source": [
    "ds_2 = ds_2.map(crear_id)\n",
    "ds_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d3cfa1-e432-44a1-ac4b-a47a662236f1",
   "metadata": {},
   "source": [
    "## Estudio del dataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f727d58-7b34-48d7-acdc-01fada796183",
   "metadata": {},
   "source": [
    "Convierto el DataSet a formato pandas, para mejor estudio de los datos y visualizaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898c2ea-dfbd-4c91-a8d4-7422734d0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603329f-ef29-4352-857e-e983a02764db",
   "metadata": {},
   "source": [
    "Busco los None del dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae54960-31e8-47b8-8911-df766f65835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0753aad-bb7c-4b8e-b29b-b7020e4d7cba",
   "metadata": {},
   "source": [
    "Observo que el dataSet no tiene ningun None, en otras palabras no tiene ningun dato faltante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eb9abe-0341-4d1f-a409-9519b5dba24c",
   "metadata": {},
   "source": [
    "Ahora voy a estudiar la longitud de los chistes, el separador sera el espacio(\" \").<br>\n",
    "La longitud de cada chiste se consegira con el dataSet tipo Arrow para optimizar la velocidad y la ram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64c8ff-7e76-4a1e-a01d-a11d23572b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longitud(chiste):\n",
    "    return {\"joke_len\": len(chiste[\"Joke\"].split())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c8b2e-b391-4673-be5f-ed1247f25691",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(longitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2c64f-f001-4b71-8869-0b26e9413027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664643c-0cad-4b6b-ac32-c039c10870ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30f31b-47b7-40b3-b50b-8d090912e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, dpi=100, figsize=(10, 7))\n",
    "sns.histplot(\n",
    "    x=\"joke_len\",\n",
    "    data=df,\n",
    "    binwidth=5,\n",
    "    color=\"#ffff00\",\n",
    ")\n",
    "axes.set_title(\"Longitud de los chistes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f48bd2-14c4-4539-a82e-43d800a7b7ee",
   "metadata": {},
   "source": [
    "Observo que hay chistes de longitud mejor a 5, los estudiare porque son posibles outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105c4ed9-dc3c-4b53-b99f-561e27e9ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "longitud_limite_chistes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d726580-8c0f-4503-95c3-36143000f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"joke_len\"] < longitud_limite_chistes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e8492-cf93-4cfe-94e0-98892ece585e",
   "metadata": {},
   "source": [
    "Me doy cuenta que si son algo chistosos por ejemplo el chiste:\n",
    "   - Canada's navy. (\"La marina de Canada\") #ID = 231575\n",
    "   - Ted Cruz getting elected. (\"Ted Cruz\") #ID = 39\n",
    "   \n",
    "No tengo nada contra Ted Cruz y Canada, pero al parecer algunas personas los consideran un chiste.<br>\n",
    "\n",
    "Vere la cantidad de chistes con, para tener una decision si eliminarlos o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18394f-684b-4876-ba2f-f6581cae70bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_a_eliminar = df[df[\"joke_len\"] < longitud_limite_chistes].shape[0]\n",
    "datos_total = df.shape[0]\n",
    "print(f\" Total de chistes: {datos_total}\\n Total de chistes con longitud menor a 5: {datos_a_eliminar}\\n Porcentaje: {(datos_a_eliminar * 100) / datos_total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d2c94-5547-4fe6-9f04-41b8a5e7885e",
   "metadata": {},
   "source": [
    "Observo que los datos con longitud menor a 5 son menos que el 1% de todo el dataSet. Decidire eliminarlos ya que no creo que aporten mucho al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f1a71-6af2-4479-bd47-4c05fa4e7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.filter(lambda x: x[\"joke_len\"] >= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb67eb8-8200-40fe-8687-48a51a6093af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311436b4-259b-4e2f-b0d7-1bb92e4bb303",
   "metadata": {},
   "source": [
    "## Preparando los dataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ad110-ba90-4614-b907-0b7dc0a090d0",
   "metadata": {},
   "source": [
    "Elimino el feature ID porque no brinda informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7b453-c134-4c91-bcec-a84aa56d2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.remove_columns([\"ID\", \"joke_len\"])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c77538-c5a0-41a8-801f-3aa67acb83b1",
   "metadata": {},
   "source": [
    "Cambio el nombre del feature \"Joke\" por \"joke\", por comodidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ced9d0-3954-4352-b882-74be5e207853",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.rename_column(\"Joke\", \"joke\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfd703-b173-4189-bb80-bc06d8772e02",
   "metadata": {},
   "source": [
    "Elimino las etiquetas HTML de los chistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1821b-c12e-49e0-84e5-f5b2c9d42cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "ds = ds.map(lambda x: {\"joke\": html.unescape(x[\"joke\"])})\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c5a0c-e745-4739-9ce9-92e6a2662d9a",
   "metadata": {},
   "source": [
    "## Dividio el dataSet en train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7e94e-74a7-46b8-be50-e1951ee3f5e2",
   "metadata": {},
   "source": [
    "Divido el dataSet en 2:\n",
    "   - Train: 70%.\n",
    "   - Test: 30%. <br>\n",
    "\n",
    "Para division sera de forma aleatoria con el parametro shuffle=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e6658-a438-4000-bce0-9d2bc6acf14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_joke = ds.train_test_split(\n",
    "                                test_size=0.3, \n",
    "                                shuffle=True,\n",
    "                                seed=1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ba582-ce22-45c9-ba07-264b3e75a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_joke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89711cb-81cd-4dfb-8314-5afc82270719",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e8661-6fa6-4136-b9f3-4bac6d290dec",
   "metadata": {},
   "source": [
    "## Tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d3780-cbb4-45f7-862f-d1e0bee61ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e815d89-6511-469e-b70f-784a42edc2a7",
   "metadata": {},
   "source": [
    "La longitud de los contextos sera de 64, por temas de optimizar la memoria(la mia es de 8gb).<br>\n",
    "Si tiene una memoria mayor y un cpu potente lo puede cambiar por 128(si usas colab),  1024 o 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e032f-ef4a-4284-8021-b5b126540324",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94381703-7dd0-4371-9083-011e920d837a",
   "metadata": {},
   "source": [
    "Longitud del contexto: Cantidad de tokens que debe tener la oracion. <br>\n",
    "#Importante: AGREGAR IMAGEN DE EXPLICACION."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60200a3-9942-42ce-a4cb-3a1b45da97a9",
   "metadata": {},
   "source": [
    "Descargo un tokenizador ya entrenado.<br>\n",
    "Cada token ya tiene establecido su propio ID <br>\n",
    "#IMPORTANTE: AGREGAR IMAGEN EXPLICANDO ESTE PROCESO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2284fc-1a37-4e8b-9eb1-d839aee5da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c1298-2102-43b5-9f93-17105ef5e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_joke = d_joke.map(lambda x: {\"joke\": html.unescape(x[\"joke\"])})\n",
    "d_joke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b208f-6e2c-4288-9c6e-85e9b63e5dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer(\n",
    "        examples[\"joke\"], \n",
    "        truncation=False,#True\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        add_special_tokens=True,\n",
    "        #max_length=15,\n",
    "    )\n",
    "    \n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        input_batch.append(input_ids + [tokenizer.eos_token_id])\n",
    "        #if length == 15:\n",
    "        #input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = d_joke.map(\n",
    "    tokenize_function, batched=True, remove_columns=d_joke[\"train\"].column_names)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00574f4-6ae8-4484-bc68-533b750e3077",
   "metadata": {},
   "source": [
    "Voy a estudiar la longitud de los tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a9005-a8eb-4cbc-be68-1ce5eff99898",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e41c4-0b12-4701-90c9-3d395a65542c",
   "metadata": {},
   "source": [
    "Observo que hay por lo menos un chiste conformado por 1024 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2dd60-0165-41ed-a423-38ad3ca28be0",
   "metadata": {},
   "source": [
    "Imprimo la cantidad de tokens de los 15 1eros chistes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b743cb5-f375-4a1c-a32a-f4453a75e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_samples = tokenized_datasets[\"train\"][:15]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c032f32e-a629-491c-8ff8-5f27dacdebbb",
   "metadata": {},
   "source": [
    "Como se menciono anteriormente, voy a trabajar con un contexto de 64 tokens, asi que debo hacer que cada chiste tenga 64 tokens ni mas ni menos.<br>\n",
    "Pero eso es un problema, ya que estamos observando que los 1eros 15 chistes estan compuestos por menos de 65 tokens.<br>\n",
    "Y tambien hemos observado que hay chistes mayores de 64 tokens.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d1e40c-b8c4-430f-bdef-d98a38d50881",
   "metadata": {},
   "source": [
    "<h3><b>¿Que se podra hacer? </b></h3>\n",
    "<p>Tenemos 4 posibles soluciones:<p>\n",
    "<ol>\n",
    "   <li> Solo quedarnos con los chistes formados por 64 tokens.</li>\n",
    "   <li> Solo quedarnos con los chistes mayores o igual a 64 tokens, y luego truncar su valores(solo quedarme con los 1ero 64 tokens).</li>\n",
    "   <li> Modificiar la longitud del contexto, un valor menor como ya vimos que los 1eros 15 chistes son de una longitud muy menor a 64, podemos cambiar el contexto a 20.\n",
    "   <li> Concatenar todos los chistes y luego dividirlo en segmentos de 64 tokens.\n",
    "</ol>\n",
    "<p><b>Problemas:</b></p>\n",
    "<ul>\n",
    "    <li> Las soluciones 1 y 2 no serian las mas convenientes para nuestro problema, ya vimos que los 1eros 15 chistes eran menores a 64 tokens, perderiamos muchos datos al solo quedarme con los chistes largos.\n",
    "    <li> Con la solucion 3 no perderia datos pero si contexto en los chistes, al solo quedarme con la parte inicial los chistes perderian significado.\n",
    "    <li> La solucion 4 es la mejor. Ahora se explicara el <b>porque</b>.\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc64dc-ed28-4574-acc9-60b36095c483",
   "metadata": {},
   "source": [
    "#IMPORTANTE: COMPLETAR <br>\n",
    "Explicacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4758485-8463-49ba-b47b-21a887375dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c7f9c-149a-429a-ace9-f2fedbea8ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = context_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ee987-8c1e-4096-8d74-1f364f469ad5",
   "metadata": {},
   "source": [
    "Una forma sencilla seria unir los tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b1616-7185-4fa8-a550-752701c01f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21f4ee-2042-42b7-bae2-41e661de7c12",
   "metadata": {},
   "source": [
    "<p> Observo que el ultimo dato, tiene 74 tokens. </p>\n",
    "<p>Tenemos 2 posibles soluciones.</p>\n",
    "<ol>\n",
    "    <li> Completar el dato con id que significan \"vacio\", para Arrow el 0 es el id vacio</li>\n",
    "    <li> ELiminar el dato</li>\n",
    "</ol>\n",
    "<p>En este caso se elegira la opcion 2.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592fd4b8-c4b9-40f5-aeeb-d66590c7e44f",
   "metadata": {},
   "source": [
    "Ahora recreare todo lo hecho con el dataSet completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad9d39-6e0d-44ed-8c46-fad1a2d060c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concateno todo el texto\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Calculo la longitud de los textos concatenados\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # Si el ultimo fragmento es mas pequenio que la \"longitud del contexto\" sera eliminado.\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Hago un split del tamanio de la \"longitud del contexto\"\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aaeea4-c2af-416e-a79d-a8b5d897af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenizado = tokenized_datasets.map(group_texts, batched=True)\n",
    "df_tokenizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a3b955-9e5d-4385-a2c0-f8506d5d4087",
   "metadata": {},
   "source": [
    "<h3>¿Por que la solucion 4 funciona?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e939e-f880-431d-9eae-47ecf7b397ae",
   "metadata": {},
   "source": [
    "<p>No solo se concatenara los chistes, sino tambien nuestro modelo gpt2(Spolier!) se dara cuenta que cada data no es un solo chiste, sino que pueden ser varios y los tomara por separado</p>\n",
    "<p><b>Y como se da cuenta cuando termina uno y comienza el siguiente chistes?</b></p>\n",
    "<p>Por el caracter especial del tokenizador eos_token, que fue agregado cuando tokenizamos en el siguiente codigo:</p>\n",
    "<ul>\n",
    "    <li>input_batch.append(input_ids + <b>[tokenizer.eos_token_id]</b>)</li>\n",
    "</ul>\n",
    "</p>Su representacion es de la siguiente forma:</p>\n",
    "<ul>\n",
    "    <li>>>> Texto: <|endoftext|> </li>\n",
    "    <li>>>> ID: 0</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe0397-c2c8-4cff-a348-3fe310348528",
   "metadata": {},
   "source": [
    "Lo vamos a observar en nuestro 1er dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de0077-86c6-4bda-88fe-9aa6ac0a073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(df_tokenizado[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918022f-9737-4e61-a947-00744fd52056",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(df_tokenizado[\"train\"][0][\"input_ids\"])\n",
    "i_cero = [i for i, dato in enumerate(ids) if dato == 0]\n",
    "print(f\"Ids del 1er dato: {ids}\")\n",
    "print(f\"Posiciones del 0: {i_cero}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d1a782-efc8-4c86-a564-8c3cc4474672",
   "metadata": {},
   "source": [
    "Observamos que en nuestro 1er dato hay 2 ceros, en otras palabras hay 3 chistes(no necesariamente el 1er y el ultimo esten completos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe2c9c-a15c-4488-a0d8-0b3d213fe0c4",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb873cab-80a3-4ea5-be39-2ba9b817e25d",
   "metadata": {},
   "source": [
    "<p>Inicializo un nuevo modelo de GPT2, con las configuracion que estoy usando.</p>\n",
    "<ul>\n",
    "    <li> La longitud de contexto en 64.</li>\n",
    "    <li> La longitud del vocabulario igual al tokenizador. </li>\n",
    "    <li> La separacion de los chistes con eos_token_id.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb867ef8-846b-463b-9a43-0811825baef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a75252-f02d-425c-ac5a-924add697d79",
   "metadata": {},
   "source": [
    "Observo los parametros a entrenar de mi modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7fe5c-2d84-454a-9bfd-8dbfd1f2bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parametros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977b3bc-ee65-47d4-a726-99951e738a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANTE: Al tener todas las filas de 128 tokens, no es necesario el recolector de datos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7895d62-2449-4b98-8375-203d2bd39279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling \n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, \n",
    "                                                mlm= False # True: Si es para un modelo de lenguaje enmascarado  \n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6fe2ac-36de-4e88-897c-6ec06afcfc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc8cf56-bc9e-4a5a-a087-234edb49fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb311815-73ff-4752-aa38-d96109844038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289b1022-e2b3-4520-a1c1-b311e0d64168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"codeparrot-ds\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    #fp16=True, #Importante: SOlo descomentar si tenes GPU\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5ffde-c3e6-4224-b4d2-2fed2a2bfab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39d044f-fb26-46ff-9a1a-b13ed3d20c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b497e0e9-cb95-45cc-bdf0-d27d064b3b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e82820b7-98b3-451c-a01a-f88279a98585",
   "metadata": {},
   "source": [
    "# Datos 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b56f7-d6c2-45ff-a7ed-fd8b7bcec877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = load_dataset(\"json\", data_files=\"./DataSets/reddit_jokes.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8fbe1-df14-46e5-8776-a1e07adedc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9132e-3e84-42d7-aa82-7b67e3ccb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d68da3-56a1-4bb2-a1bb-310802a6376f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f9597-e08e-4c58-a0e1-6a68fd8eb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3a51d-b49b-4568-bac2-2d838c3b489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.remove_columns([\"score\", \"title\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0060c-46b3-4823-ae46-41e0d7374a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e534281-ffb3-4440-bfde-ba2944d2d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255047ec-cf78-4141-ae1d-67c979c06075",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68732b15-9f63-4af4-b3ea-0f3ebd1a48f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
